#!/usr/bin/env ruby

require 'optparse'
require 'etc'
require 'json'
require 'yaml'
require 'fcntl'
require 'fileutils'
require 'faye/websocket'
require 'eventmachine'

# Load common libraries
require_relative 'lib/jwt'
require_relative 'lib/metrics'
require_relative 'lib/job_tracker'
require_relative 'docker/docker'
require_relative 'qemu/qemu'
require_relative '../container/defconfig'

names = Set.new %w[
  SCHED_HOST
  SCHED_PORT
  DOMAIN_NAME
]

defaults = relevant_defaults(names)
SCHED_HOST = ENV['SCHED_HOST'] || defaults['SCHED_HOST'] || '172.17.0.1'
SCHED_PORT = ENV['SCHED_HOST'] || defaults['SCHED_PORT'] || 3000

DOMAIN_NAME = defaults['DOMAIN_NAME']

ARCH = `arch`.chomp
NR_NODE = Dir.glob('/sys/devices/system/node/node*').count
NR_CPU = Etc.nprocessors
NR_DISKS = count_disks_sys_block

BASE_DIR = Process.uid.zero? ? "/srv" : "#{ENV['HOME']}/.cache/cci"

class MultiQemuDocker
  DEFAULT_OPTIONS = {
    hostname: ENV['HOST'] || ENV['HOSTNAME'] || `hostname`.chomp,
    tbox_type: 'vm,dc',
    max_qemu: 0,
    max_container: 0,
    is_remote: false
  }.freeze

  MIN_QEMU_CPU = 2
  MIN_CONTAINER_CPU = 2
  MIN_QEMU_MEMORY = 4       # GB
  MIN_CONTAINER_MEMORY = 8  # GB

  HEARTBEAT_INTERVAL = 10   # seconds
  MEM_CHECK_INTERVAL = 1    # second
  MEM_DELTA_THRESHOLD = 100 # 100MB

  JOB_DONE_FIFO_PATH = "/tmp/job_completion_fifo"

  attr_reader :options, :scheduler_url, :scheduler_ws_url
  attr_accessor :last_free_mem, :last_status_time

  def initialize(argv)
    @options = DEFAULT_OPTIONS.merge(parse_cli_options(argv))
    @last_meminfo = parse_meminfo
    @last_free_mem = free_memory_mb
    @last_status_time = Time.now
    setup_directories
    update_options
    @scheduler_ws_url = build_scheduler_ws_url
    @jobs = JobTracker.new
    @fifo_thread = start_fifo_listener
    @reconnect_attempts = 0
  end

  def setup_directories
    File.umask(0000) # Ensure directories are created with proper permissions
    FileUtils.mkdir_p(ENV["HOSTS_DIR"] = "#{BASE_DIR}/provider/hosts")
    FileUtils.mkdir_p(ENV["JOBS_DIR"] = "#{BASE_DIR}/provider/jobs")
    FileUtils.mkdir_p(ENV["PIDS_DIR"] = "#{BASE_DIR}/provider/pids")
    FileUtils.mkdir_p(ENV["OGS_DIR"] = "#{BASE_DIR}/provider/logs")
  end

  def update_options
    if @options[:max_qemu] == 0
      @options[:max_qemu] = compute_max_vm
    end
    if @options[:max_container] == 0
      @options[:max_container] = compute_max_dc
    end
  end

  def create_fifo
    unless File.exist?(JOB_DONE_FIFO_PATH)
      system('mkfifo', JOB_DONE_FIFO_PATH)
    end
  end

  def start_fifo_listener
    create_fifo

    Thread.new do
      loop do
        # Need re-open FIFO for block reading, since the other side always
        # close FIFO after writing one line of job_id.
        File.open(JOB_DONE_FIFO_PATH, 'r') do |fifo|
          job_id = fifo.gets
          if job_id
            job_id.chomp!
            @jobs.remove_job(job_id)
            puts "Job #{job_id} completed and removed from the job list."
          else
            puts "Unexpected end of input from the FIFO."
          end
        end
      end
    end

  end

  def open_websocket(url)
    if @options[:is_remote]
      jwt = load_jwt?
      ws = Faye::WebSocket::Client.new(url, [], :headers => { 'Authorization' => jwt })
    else
      ws = Faye::WebSocket::Client.new(url)
    end
    ws
  end

  def start_event_machine
    begin
      EM.run do
        puts "Initializing new connection..."
        @ws = open_websocket(@scheduler_ws_url)

        @ws.on(:open) do
          puts "WebSocket connection opened"
          @reconnect_attempts = 0
          send_status
          start_heartbeat
          start_mem_monitoring
        end

        @ws.on(:message) do |event|
          puts "Received: #{event.data}"
          handle_websocket_message(event.data)
        rescue => e
          puts "Message handling error: #{e.message}"
          puts e.backtrace.join("\n")
        end

        @ws.on(:close) do |event|
          puts "Connection closed with code #{event.code}"
          EM.stop_event_loop
        end

        @ws.on(:error) do |event|
          puts "WebSocket error: #{event.message}"
          puts "Please check if server is running on #{@scheduler_ws_url}" if event.message == Errno::ECONNREFUSED
          EM.stop_event_loop

          @reconnect_attempts += 1
          delay = [2 ** @reconnect_attempts, 30].min
          puts "Attempting reconnect in #{delay} second..."
          sleep(delay)
        end
      end
    rescue => e
      puts "Connection failed: #{e.message}"
    ensure
      # Cleanup resources if needed
      @ws = nil
    end
  end

  private

  def parse_cli_options(argv)
    options = {}

    OptionParser.new do |opts|
      opts.banner = 'Usage: multi-qemu-docker [options]'

      opts.on('-n HOSTNAME', '--name HOSTNAME', 'Hostname for reporting') { |v| options[:hostname] = v }
      opts.on('-t TYPE', '--tbox-type TYPE', 'Testbox type (vm/dc, or both by default)') { |v| options[:tbox_type] = v }
      opts.on('--max-qemu COUNT', 'Number of QEMU instances (0 for auto-computing)') { |v| options[:max_qemu] = v }
      opts.on('--max-container COUNT', 'Number of container instances (0 for auto-computing)') { |v| options[:max_container] = v }
      opts.on('-t tags', '--tags TAGS', 'separated by ","') { |v| opt['tags'] = v }
      opts.on('-r', '--remote', 'Testbox is remote') { options[:is_remote] = true }
      opts.on('-h', '--help', 'Show this message') { puts opts; exit }
    end.parse!(argv)

    options
  end

  def compute_max_vm
    host_cpu = Etc.nprocessors
    host_mem = total_memory_mb >> 10

    nr_vm_on_cpu = host_cpu / MIN_QEMU_CPU
    nr_vm_on_mem = host_mem / MIN_QEMU_MEMORY

    puts "nr_vm_on_cpu: #{nr_vm_on_cpu}"
    puts "nr_vm_on_mem: #{nr_vm_on_mem}"

    max_vm = [nr_vm_on_cpu, nr_vm_on_mem].min
  end

  def compute_max_dc
    Etc.nprocessors / MIN_CONTAINER_CPU
  end

  def build_scheduler_ws_url
    if @options[:is_remote]
      "wss://#{DOMAIN_NAME}/scheduler/vm-container-provider/#{@options[:hostname]}"
    else
      "ws://#{SCHED_HOST}:#{SCHED_PORT}/scheduler/vm-container-provider/#{@options[:hostname]}"
    end
  end

  def send_status
    now = Time.now
    return if @prev_io_timestamp && now - @prev_io_timestamp < 1

    # Pre-calculate values
    cpu_stats = cpu_metrics(now)
    disk_usage = disk_max_used_percent
    network_stats = network_metrics(now)

    status = {
      type: 'host-job-request',
      hostname: @options[:hostname],
      arch: ARCH,
      nr_cpu: NR_CPU,
      nr_node: NR_NODE,
      nr_disks: NR_DISKS,
      nr_vm: JobTracker.nr_vm,
      nr_container: JobTracker.nr_container,
      tbox_type: @options[:tbox_type],
      is_remote: @options[:is_remote],
      tags: [],

      freemem: free_memory_mb, # duplicate for fast access in HostRequest
      disk_max_used_string: disk_usage[:string], # "92% /srv/os"
      metrics: { # must be number values
        freemem: free_memory_mb,
        freemem_percent: (100 * free_memory_mb) / total_memory_mb, # Integer division
        disk_max_used_percent: disk_usage[:value], # 92
        cpu_idle_percent: cpu_stats[:idle],
        cpu_iowait_percent: cpu_stats[:iowait],
        cpu_system_percent: cpu_stats[:system],
        disk_io_util_percent: disk_io_utilization(now),
        network_util_percent: network_stats[:utilization],
        network_errors_per_sec: network_stats[:errors],
        uptime_minutes: (File.read('/proc/uptime').split[0].to_i / 60), # Integer division
      }
    }
    # pp status
    @ws.send(status.to_json)
  end

  # report status every 10 seconds
  def start_heartbeat
    EM.add_periodic_timer(HEARTBEAT_INTERVAL) { send_status }
  end

  def start_mem_monitoring
    EM.add_periodic_timer(MEM_CHECK_INTERVAL) do
      @last_meminfo = parse_meminfo
      current_mem = free_memory_mb
      if (current_mem - @last_free_mem).abs > MEM_DELTA_THRESHOLD || Time.now - @last_status_time > HEARTBEAT_INTERVAL
        @last_free_mem = current_mem
        send_status
        @last_status_time = Time.now
      end
    end
  end

  def handle_websocket_message(data)
    message = JSON.parse(data)
    case message['type']
    when 'boot-job'
      boot_job(message)
    when 'terminate-job'
      terminate_job(message)

    when 'watch-job-log'
      watch_log(message)
    when 'unwatch-job-log'
      unwatch_log(message)

    when 'request-console'
      start_console_session(message)
    when 'console-input'
      write_to_console(message)
    when 'close-console'
      close_console_session(message)

    else
      puts "Unknown message type: #{message['type']}"
    end
  end

  def boot_job(message)
    job_id = message['job_id']
    puts "Dispatching job: #{message}"

    hostname = @jobs.find_hostname(message['tbox_group'])
    message['hostname'] = hostname

    fork_pid = Process.fork do
      ENV["hostname"] = hostname
      ENV["host_dir"] = "#{ENV["HOSTS_DIR"]}/#{hostname}"
      ENV["log_file"] = "#{ENV["LOGS_DIR"]}/#{hostname}"
      ENV["is_remote"] = @options[:is_remote].to_s

      case message['tbox_type']
      when 'vm', 'qemu'
        QemuManager.new(message).start_qemu_instance
      when 'dc', 'container'
        DockerManager.new(message).start_container_instance
      end

      # Write job_id to FIFO when job is done
      File.open(JOB_DONE_FIFO_PATH, 'a') do |fifo|
        fifo.puts job_id
      end
    end

    if fork_pid
      Process.detach(fork_pid)
      job = {
        id: job_id,
        tbox_type: message['tbox_type'],
        hostname: hostname,
        fork_pid: fork_pid
      }
      @jobs.add_job(job)
    end
  end

  def terminate_job(message)
    job_id = message['job_id']
    puts "Canceling job: #{job_id}"
    @jobs.terminate_job(job_id)
  end

  def watch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    log_file = "#{ENV["LOGS_DIR"]}/#{job[:hostname]}.log"

    # Create a pipe to capture the output of the tail process
    reader, writer = IO.pipe

    # Start the tail process using spawn
    tail_pid = spawn("tail --pid #{job[:fork_pid]} -F #{log_file}", out: writer)
    # Process.detach(tail_pid) # avoid zombie processes

    # Store the tail process PID and pipe reader
    @jobs[job_id][:tail_pid] = tail_pid
    @jobs[job_id][:tail_reader] = reader

    # Read from the pipe in a separate thread
    Thread.new do
      begin
        while line = reader.gets
          # Send log output to WebSocket
          @ws.send({ type: 'job-log', job_id: job_id, data: line }.to_json)
        end
      rescue IOError => e
        puts "Error reading from tail process for job #{job_id}: #{e.message}"
      ensure
        reader.close
        writer.close
        Process.wait(tail_pid) # avoid zombie processes
      end
    end
  end

  def unwatch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]

    if job && job[:tail_pid]
      # Terminate the tail process
      Process.kill("TERM", job[:tail_pid])

      # Close the pipe
      if job[:tail_reader]
        job[:tail_reader].close
        job.delete(:tail_reader)
      end

      # Clean up the stored PID
      job.delete(:tail_pid)
      puts "Stopped log watching for job #{job_id}"
    else
      puts "No log watching process found for job #{job_id}"
    end
  end

  def start_console_session(message)
    job_id = message['job_id']
    testbox_type = message['testbox_type']
    job = @jobs[job_id]

    return if job[:console_io]

    if testbox_type == 'qemu'
      start_qemu_console(job_id, job)
    elsif testbox_type == 'docker'
      start_docker_console(job_id, job)
    end
  end

  def start_qemu_console(job_id, job)
    socket_path = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/qemu-console.sock"
    begin
      socket = UNIXSocket.new(socket_path)
      job[:console_io] = socket

      # Thread to read from QEMU socket and send output
      job[:console_thread] = Thread.new do
        loop do
          output = socket.readpartial(1024)
          @ws.send({ type: 'console-output', job_id: job_id, data: output }.to_json)
        rescue => e
          break
        end
      end
    rescue => e
      puts "Failed to attach to QEMU console: #{e.message}"
    end
  end

  def start_docker_console(job_id, job)
    container_id = job_id
    job[:console_io] = nil

    begin
      job[:console_thread] = Thread.new do
        PTY.spawn("docker", "exec", "-it", container_id, "/bin/bash") do |output, input, pid|
          job[:console_io] = input
          job[:docker_pid] = pid

          begin
            loop do
              data = output.readpartial(1024)
              @ws.send({ type: 'console-output', job_id: job_id, data: data }.to_json)
            end
          rescue EOFError, Errno::EIO
            # Process exited normally
          rescue => e
            puts "Docker console read error: #{e}"
          ensure
            input.close rescue nil
            output.close rescue nil
            job.delete(:console_io)
            job.delete(:docker_pid)
          end
        end
      rescue => e
        puts "Failed to start Docker console: #{e}"
      end
    end
  end

  def write_to_console(message)
    job_id = message['job_id']
    data = message['data']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    begin
      job[:console_io].write(data)
      job[:console_io].flush
    rescue => e
      puts "Error writing to console: #{e}"
    end
  end

  def close_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    begin
      job[:console_io].close
    rescue => e
      puts "Error closing console IO: #{e}"
    end

    if job[:docker_pid]
      begin
        Process.kill('TERM', job[:docker_pid])
      rescue => e
        puts "Error killing Docker process: #{e}"
      end
    end

    job[:console_thread].kill if job[:console_thread]
    job.delete(:console_io)
    job.delete(:console_thread)
    job.delete(:docker_pid)
  end

  def update_code(commit_id)
    # if there is no commit_id
    # the code is not updated
    return unless commit_id

    dir = "#{ENV['CCI_SRC']}/in-pull"
    return unless File.exist? dir
    FileUtils.mkdir(dir) rescue return

    cmd = "cd #{ENV['CCI_SRC']};git pull;git reset --hard #{commit_id}"
    puts cmd
    system(cmd)
  ensure
    FileUtils.rmdir(dir)
  end

  def safe_stop
    # Graceful shutdown logic
    EventMachine.stop
  end

end

if __FILE__ == $PROGRAM_NAME
  provider = MultiQemuDocker.new(ARGV)
  loop do
    provider.start_event_machine
    sleep 1
  end
end
