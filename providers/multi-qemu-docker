#!/usr/bin/env ruby

require 'optparse'
require 'etc'
require 'json'
require 'yaml'
require 'rbconfig'
require 'fileutils'
require 'faye/websocket'
require 'eventmachine'
require 'docker-api'

# Load common libraries
require_relative 'lib/jwt'
require_relative 'lib/job_tracker'
require_relative 'docker/docker'
require_relative 'qemu/qemu'
require_relative '../container/defconfig'

names = Set.new %w[
  SCHED_HOST
  SCHED_PORT
  DOMAIN_NAME
]

defaults = relevant_defaults(names)
SCHED_HOST = ENV['SCHED_HOST'] || defaults['SCHED_HOST'] || '172.17.0.1'
SCHED_PORT = ENV['SCHED_HOST'] || defaults['SCHED_PORT'] || 3000

DOMAIN_NAME = defaults['DOMAIN_NAME']

class MultiQemuDocker
  DEFAULT_OPTIONS = {
    hostname: ENV['HOSTNAME'] || 'default-host',
    tbox_type: 'qemu,container',
    max_qemu: 0,
    max_container: 0,
    is_remote: false
  }.freeze

  MIN_QEMU_CPU = 2
  MIN_CONTAINER_CPU = 2
  MIN_QEMU_MEMORY = 4       # GB
  MIN_CONTAINER_MEMORY = 8  # GB

  HEARTBEAT_INTERVAL = 10   # seconds
  MEM_CHECK_INTERVAL = 1    # second
  MEM_DELTA_THRESHOLD = 100 # 100MB

  JOB_DONE_FIFO_PATH = "/tmp/job_completion_fifo"

  attr_reader :options, :scheduler_url, :scheduler_ws_url
  attr_accessor :last_free_mem, :last_status_time

  def initialize(argv)
    @options = DEFAULT_OPTIONS.merge(parse_cli_options(argv))
    @last_meminfo = parse_meminfo
    @last_free_mem = free_memory_mb
    @last_status_time = Time.now
    setup_directories
    update_options
    @scheduler_ws_url = build_scheduler_ws_url
    @jobs = JobTracker.new
    @fifo_thread = start_fifo_listener
  end

  def setup_directories
    File.umask(0000) # Ensure directories are created with proper permissions
    FileUtils.mkdir_p(ENV["HOSTS_DIR"] = "/srv/cci/hosts")
    FileUtils.mkdir_p(ENV["JOBS_DIR"] = "/srv/cci/jobs")
    FileUtils.mkdir_p(ENV["PIDS_DIR"] = "/srv/cci/pids")
    FileUtils.mkdir_p(ENV["LOGS_DIR"] = "/srv/cci/logs")
  end

  def update_options
    if @options[:max_qemu] == 0
      @options[:max_qemu] = compute_max_vm
    end
    if @options[:max_container] == 0
      @options[:max_container] = compute_max_dc
    end
  end

  def create_fifo
    unless File.exist?(JOB_DONE_FIFO_PATH)
      FileUtils.mkfifo(JOB_DONE_FIFO_PATH)
    end
  end

  def start_fifo_listener
    create_fifo

    Thread.new do
      File.open(FIFO_PATH, 'r') do |fifo|
        loop do
          job_id = fifo.gets.chomp
          @jobs.remove_job(job_id)
          puts "Job #{job_id} completed and removed from the job list."
        end
      end
    end
  end

  def open_websocket(url)
    if is_remote.to_s == 'true'
      jwt = load_jwt?
      ws = Faye::WebSocket::Client.new(url, [], :headers => { 'Authorization' => jwt })
    else
      ws = Faye::WebSocket::Client.new(url)
    end
    ws
  end

  def start_event_machine
    EM.run do
      @ws = open_websocket(@scheduler_ws_url)

      @ws.on(:open) do
        puts "WebSocket connection opened"
        send_status
        start_heartbeat
        start_mem_monitoring
      end

      @ws.on(:message) do |event|
        puts "Received: #{event.data}"
        handle_websocket_message(event.data)
      end

      @ws.on(:close) do |event|
        puts "WebSocket closed with code #{event.code}"
        safe_stop
        EM.stop
      end
    end
  rescue => e
    puts "WebSocket error: #{e.message}"
    retry
  end


  private

  def parse_cli_options(argv)
    options = {}

    OptionParser.new do |opts|
      opts.banner = 'Usage: multi-qemu-docker [options]'

      opts.on('-n HOSTNAME', '--name HOSTNAME', 'Hostname for reporting') { |v| options[:hostname] = v }
      opts.on('-t TYPE', '--tbox-type TYPE', 'Testbox type (qemu, container, or both by default)') { |v| options[:tbox_type] = v }
      opts.on('--max-qemu COUNT', 'Number of QEMU instances (0 for auto-computing)') { |v| options[:max_qemu] = v }
      opts.on('--max-container COUNT', 'Number of container instances (0 for auto-computing)') { |v| options[:max_container] = v }
      opts.on('-t tags', '--tags TAGS', 'separated by ","') { |v| opt['tags'] = v }
      opts.on('-r', '--remote', 'Testbox is remote') { options[:is_remote] = true }
      opts.on('-h', '--help', 'Show this message') { puts opts; exit }
    end.parse!(argv)

    options
  end

  def compute_max_vm
    host_cpu = Etc.nprocessors
    host_mem = total_memory_mb >> 10

    nr_vm_on_cpu = host_cpu / MIN_QEMU_CPU
    nr_vm_on_mem = host_mem / MIN_QEMU_MEMORY

    puts "nr_vm_on_cpu: #{nr_vm_on_cpu}"
    puts "nr_vm_on_mem: #{nr_vm_on_mem}"

    max_vm = [nr_vm_on_cpu, nr_vm_on_mem].min
  end

  def compute_max_dc
    Etc.nprocessors / MIN_CONTAINER_CPU
  end

  def build_scheduler_ws_url
    if @options[:is_remote]
      "wss://#{DOMAIN_NAME}/ws/vm-container-provider/#{@options[:hostname]}"
    else
      "ws://#{SCHED_HOST}:#{SCHED_PORT}/ws/vm-container-provider/#{@options[:hostname]}"
    end
  end

  def send_status
    status = {
      type: 'host-job-request',
      host_machine: @options[:hostname],
      freemem: free_memory_mb,
      tbox_type: @options[:tbox_type],
      is_remote: @options[:is_remote],
      arch: RbConfig::CONFIG['host_cpu']
      tags: []
    }
    @ws.send(status.to_json)
  end

  def total_memory_mb
    @last_meminfo["MemTotal"]
  end

  def free_memory_mb
    @last_meminfo["MemFree"]
  end

  def parse_meminfo
    meminfo_hash = {}
    File.open('/proc/meminfo', 'r') do |file|
      file.each_line do |line|
        key, value = line.split(':')
        meminfo_hash[key.strip] = value.strip.to_i >> 10 # Convert bytes to MiB
      end
    end
    meminfo_hash
  end

  def start_heartbeat
    EM.add_periodic_timer(HEARTBEAT_INTERVAL) { send_status }
  end

  def start_mem_monitoring
    EM.add_periodic_timer(MEM_CHECK_INTERVAL) do
      @last_meminfo = parse_meminfo
      current_mem = free_memory_mb
      if (current_mem - @last_free_mem).abs > MEM_DELTA_THRESHOLD || Time.now - @last_status_time > HEARTBEAT_INTERVAL
        @last_free_mem = current_mem
        send_status
        @last_status_time = Time.now
      end
    end
  end

  def handle_websocket_message(data)
    message = JSON.parse(data)
    case message['type']
    when 'boot-job'
      boot_job(message)
    when 'terminate-job'
      terminate_job(message)

    when 'watch-job-log'
      watch_log(message)
    when 'unwatch-job-log'
      unwatch_log(message)

    when 'request-console'
      start_console_session(message)
    when 'console-input'
      write_to_console(message)
    when 'close-console'
      close_console_session(message)

    else
      puts "Unknown message type: #{message['type']}"
    end
  end

  def boot_job(message)
    job_id = message['job_id']
    puts "Dispatching job: #{message}"

    hostname = @jobs.find_hostname(message['tbox_group'])
    message['hostname'] = hostname

    fork_pid = Process.fork do
      ENV["hostname"] = hostname
      ENV["host_dir"] = "#{ENV["HOSTS_DIR"]}/#{hostname}"
      ENV["log_file"] = "#{ENV["LOGS_DIR"]}/#{hostname}"
      ENV["is_remote"] = @options[:is_remote].to_s

      case message['tbox_type']
      when 'qemu'
        QemuManager.new(message).start_qemu_instance
      when 'container'
        start_container_instance(message)
      end

      # Write job_id to FIFO when job is done
      File.open(JOB_DONE_FIFO_PATH, 'a') do |fifo|
        fifo.puts job_id
      end
    end

    if fork_pid
      Process.detach(fork_pid)
      job = {
        id: job_id,
        tbox_type: message['tbox_type'],
        hostname: hostname,
        fork_pid: fork_pid
      }
      @jobs.add_job(job)
    end
  end

  def terminate_job(message)
    job_id = message['job_id']
    puts "Canceling job: #{job_id}"
    @jobs.terminate_job(job_id)
  end

  def watch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    log_file = "#{ENV["LOGS_DIR"]}/#{job[:hostname]}.log"

    # Create a pipe to capture the output of the tail process
    reader, writer = IO.pipe

    # Start the tail process using spawn
    tail_pid = spawn("tail --pid #{job[:fork_pid]} -F #{log_file}", out: writer)
    # Process.detach(tail_pid) # avoid zombie processes

    # Store the tail process PID and pipe reader
    @jobs[job_id][:tail_pid] = tail_pid
    @jobs[job_id][:tail_reader] = reader

    # Read from the pipe in a separate thread
    Thread.new do
      begin
        while line = reader.gets
          # Send log output to WebSocket
          @ws.send({ type: 'job-log', job_id: job_id, data: line }.to_json)
        end
      rescue IOError => e
        puts "Error reading from tail process for job #{job_id}: #{e.message}"
      ensure
        reader.close
        writer.close
        Process.wait(tail_pid) # avoid zombie processes
      end
    end
  end

  def unwatch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]

    if job && job[:tail_pid]
      # Terminate the tail process
      Process.kill("TERM", job[:tail_pid])

      # Close the pipe
      if job[:tail_reader]
        job[:tail_reader].close
        job.delete(:tail_reader)
      end

      # Clean up the stored PID
      job.delete(:tail_pid)
      puts "Stopped log watching for job #{job_id}"
    else
      puts "No log watching process found for job #{job_id}"
    end
  end

  def tail_log(job_id)
    # Log streaming logic using WebSocket
    system("tail -f /srv/cci/serial/logs/#{job_id}.log")
  end

  def start_console_session(message)
    job_id = message['job_id']
    testbox_type = message['testbox_type']
    job = @jobs[job_id]

    return if job[:console_io]

    if testbox_type == 'qemu'
      start_qemu_console(job_id, job)
    elsif testbox_type == 'docker'
      start_docker_console(job_id, job)
    end
  end

  def start_qemu_console(job_id, job)
    socket_path = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/qemu-console.sock"
    begin
      socket = UNIXSocket.new(socket_path)
      job[:console_io] = socket

      # Thread to read from QEMU socket and send output
      job[:console_thread] = Thread.new do
        loop do
          output = socket.readpartial(1024)
          @ws.send({ type: 'console-output', job_id: job_id, data: output }.to_json)
        rescue => e
          break
        end
      end
    rescue => e
      puts "Failed to attach to QEMU console: #{e.message}"
    end
  end

  def start_docker_console(job_id, job)
    container_id = job_id
    exec = Docker::Exec.create(container_id, cmd: '/bin/bash', tty: true, stdin: true)
    stream = Docker::Exec.start(exec, tty: true, stdin: true, socket: true)

    job[:console_io] = stream

    # Thread to read from docker exec stream
    job[:console_thread] = Thread.new do
      while data = stream.read(1024)
        @ws.send({ type: 'console-output', job_id: job_id, data: data }.to_json)
      end
    rescue => e
      puts "Docker console error: #{e}"
    end
  end

  def write_to_console(message)
    job_id = message['job_id']
    data = message['data']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    begin
      job[:console_io].write(data)
      job[:console_io].flush
    rescue => e
      puts "Error writing to console: #{e}"
    end
  end

  def close_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    job[:console_io].close
    job[:console_thread].kill if job[:console_thread]
    job.delete(:console_io)
    job.delete(:console_thread)
  end

  def update_code(commit_id)
    # if there is no commit_id
    # the code is not updated
    return unless commit_id

    dir = "#{ENV['CCI_SRC']}/in-pull"
    return unless File.exist? dir
    FileUtils.mkdir(dir) rescue return

    cmd = "cd #{ENV['CCI_SRC']};git pull;git reset --hard #{commit_id}"
    puts cmd
    system(cmd)
  ensure
    FileUtils.rmdir(dir)
  end

  def safe_stop
    # Graceful shutdown logic
    EventMachine.stop
    system("systemctl stop #{ENV['suite']}.service")
  end

end

MultiQemuDocker.new(ARGV).start_event_machine if __FILE__ == $PROGRAM_NAME

