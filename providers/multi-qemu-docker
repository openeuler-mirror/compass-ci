#!/usr/bin/env ruby

require 'optparse'
require 'etc'
require 'json'
require 'yaml'
require 'fcntl'
require 'fileutils'
require 'find'
require 'open3'
require 'pty'
require 'shellwords'
require 'faye/websocket'
require 'eventmachine'

# Load common libraries
require_relative 'lib/jwt'
require_relative 'lib/metrics'
require_relative 'lib/job_tracker'
require_relative 'docker/docker'
require_relative 'qemu/qemu'
require_relative '../container/defconfig'

names = Set.new %w[
  SCHED_HOST
  SCHED_PORT
  DOMAIN_NAME
]

defaults = relevant_defaults(names)
SCHED_HOST = ENV['SCHED_HOST'] || defaults['SCHED_HOST'] || '172.17.0.1'
SCHED_PORT = ENV['SCHED_PORT'] || defaults['SCHED_PORT'] || 3000

DOMAIN_NAME = defaults['DOMAIN_NAME']

ARCH = `arch`.chomp
NR_NODE = Dir.glob('/sys/devices/system/node/node*').count
NR_CPU = Etc.nprocessors
NR_DISKS = count_disks_sys_block

BASE_DIR = Process.uid.zero? ? "/var/lib/docker/compass-ci" : "#{ENV['HOME']}/.cache/compass-ci"
CACHE_LOCK_ROOT = "#{BASE_DIR}/.locks"

class MultiQemuDocker
  DEFAULT_OPTIONS = {
    hostname: ENV['HOST'] || ENV['HOSTNAME'] || `hostname`.chomp,
    tbox_type: 'vm,dc',
    max_qemu: 0,
    max_container: 0,
    is_remote: false,
    tags: [],
  }.freeze

  MIN_QEMU_CPU = 2
  MIN_CONTAINER_CPU = 2
  MIN_QEMU_MEMORY = 4       # GB
  MIN_CONTAINER_MEMORY = 8  # GB

  HEARTBEAT_INTERVAL = 10   # seconds
  MEM_CHECK_INTERVAL = 1    # second
  MEM_DELTA_THRESHOLD = 100 # 100MB

  JOB_DONE_FIFO_PATH = "/tmp/job_completion_fifo"

  attr_reader :options, :scheduler_url, :scheduler_ws_url
  attr_accessor :last_free_mem, :last_status_time

  def initialize(argv)
    @options = DEFAULT_OPTIONS.merge(parse_cli_options(argv))
    @last_meminfo = parse_meminfo
    @last_free_mem = free_memory_mb
    @last_status_time = Time.now
    setup_directories
    update_options
    @scheduler_ws_url = build_scheduler_ws_url
    @jobs = JobTracker.new
    @fifo_thread = start_fifo_listener
    @reconnect_attempts = 0
    @container_runtime = `command -v podman || command -v docker`.chomp
    @cache_dirs = []
  end

  def setup_directories
    File.umask(0000) # Ensure directories are created with proper permissions
    FileUtils.mkdir_p(ENV["HOSTS_DIR"] = "#{BASE_DIR}/provider/hosts")
    FileUtils.mkdir_p(ENV["JOBS_DIR"] = "#{BASE_DIR}/provider/jobs")
    FileUtils.mkdir_p(ENV["PIDS_DIR"] = "#{BASE_DIR}/provider/pids")
    FileUtils.mkdir_p(ENV["LOGS_DIR"] = "#{BASE_DIR}/provider/logs")
    FileUtils.mkdir_p(ENV["DOWNLOAD_DIR"] = "#{BASE_DIR}/provider/download")
    File.umask(0000)
    FileUtils.mkdir_p(CACHE_LOCK_ROOT)
    FileUtils.mkdir_p(ENV["CACHE_DIR"] = "#{BASE_DIR}/provider/cache")
    FileUtils.mkdir_p(ENV["PACKAGE_CACHE_DIR"] = "#{BASE_DIR}/provider/pkgcache")
  end

  def update_options
    if @options[:max_qemu] == 0
      @options[:max_qemu] = compute_max_vm
    end
    if @options[:max_container] == 0
      @options[:max_container] = compute_max_dc
    end
  end

  def create_fifo
    unless File.exist?(JOB_DONE_FIFO_PATH)
      system('mkfifo', JOB_DONE_FIFO_PATH)
    end
  end

  def start_fifo_listener
    create_fifo

    Thread.new do
      loop do
        # Need re-open FIFO for block reading, since the other side always
        # close FIFO after writing one line of job_id.
        File.open(JOB_DONE_FIFO_PATH, 'r') do |fifo|
          job_id = fifo.gets
          unless job_id
            puts "Unexpected end of input from the FIFO."
            next
          end
          case job_id.chomp
            when /^boot: (\d+)/
              job_id = $1
              status = {
                type: 'job-update',
                job_id: job_id,
                job_stage: "boot",
              }
              @ws.send(status.to_json)
            when /^done: (\d+)/
              job_id = $1
              job = @jobs.remove_job(job_id)

              upload_log(job)
              upload_results(job)
              status = {
                type: 'job-update',
                job_id: job_id,
                job_data_readiness: "uploaded",  # trigger extract-stats
              }
              @ws.send(status.to_json)

              puts "Job #{job_id} completed and removed from the job list."
          end
        end
      end
    end

  end

  # ruby version of $LKP_SRC/lib/upload.sh upload_one()
  def upload_one(job, local_file, remote_path)
      # URL encode the remote path (important for spaces, etc.)
      encoded_remote_path = remote_path.gsub(/([^a-zA-Z0-9_.\/-])/) { |match| "%#{match.ord.to_s(16).upcase}" }

      # Construct the target path
      target_path = File.join(job[:result_root], encoded_remote_path)

      # Construct the HTTP URL
      http_url = "http://#{SCHED_HOST}:#{SCHED_PORT}#{target_path}?job_id=#{job[:id]}&job_token=#{job[:job_token]}"

      # Maximum retries and retry delay
      max_retries = 50
      retry_delay = 5
      retry_count = 0

      # Read the file content in binary mode
      file_content = File.binread(local_file)

      # Retry loop
      loop do
        # Use Open3 to execute curl with stdin_data
        stdout, stderr, status = Open3.capture3(
          'curl', '-s', '--fail-with-body', '-X', 'POST', '-w', '\ncode=%{http_code}', '--data-binary', '@-', http_url,
          stdin_data: file_content
        )

        # Extract the HTTP status code from the output
        response_code = stdout.match(/code=(\d+)/)[1].to_i

        case response_code
        when 200, 201
          break
        when 400..600
          puts "Upload failed: #{local_file} to #{target_path}"
          puts stdout
          break
        else
          if retry_count < max_retries
            puts "Upload failed (#{stdout}). Retrying in #{retry_delay} seconds..."
            sleep retry_delay
            retry_count += 1
            retry_delay *= 2
          else
            puts "Upload failed after #{max_retries} attempts"
            break
          end
        end
      end
  end

  def upload_log(job)
    upload_one(job, "#{ENV["LOGS_DIR"]}/#{job[:hostname]}", "console.log")
  end

  def upload_results(job)
    # Construct the base directory path
    base_dir = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/result_root/"

    # Recursively find all files under the base directory
    Find.find(base_dir) do |local_file|
      next unless File.file?(local_file) # Skip directories, only process files

      # Construct the remote path by preserving the local pathname/filename under result_root
      remote_path = local_file.sub(base_dir, '')

      upload_one(job, local_file, remote_path)
    end
  end

  def open_websocket(url)
    if @options[:is_remote]
      jwt = load_jwt?
      ws = Faye::WebSocket::Client.new(url, [], :headers => { 'Authorization' => jwt })
    else
      ws = Faye::WebSocket::Client.new(url)
    end
    ws
  end

  def start_event_machine
    begin
      EM.run do
        EM.set_quantum(10)
        EM.epoll
        # EM.set_tcp_nodelay(true)
        puts "Initializing new connection..."
        @ws = open_websocket(@scheduler_ws_url)
        # no such method
        # @ws.socket.setsockopt(Socket::IPPROTO_TCP, Socket::TCP_NODELAY, 1)

        @ws.on(:open) do
          puts "WebSocket connection opened"
          @reconnect_attempts = 0
          send_status
          start_heartbeat
          start_cache_reclaim
          start_mem_monitoring
        end

        @ws.on(:message) do |event|
          # start_time = Time.now.utc
          handle_websocket_message(event.data)
          # elapsed_time = Time.now.utc - start_time
          # puts "Message processing took #{elapsed_time * 1000} ms"
        rescue => e
          puts "Message handling error: #{e.message}"
          puts e.backtrace.join("\n")
        end

        @ws.on(:close) do |event|
          puts "Connection closed with code #{event.code}"
          EM.stop_event_loop
        end

        @ws.on(:error) do |event|
          puts "WebSocket error: #{event.message}"
          puts "Please check if server is running on #{@scheduler_ws_url}" if event.message == Errno::ECONNREFUSED
          EM.stop_event_loop

          @reconnect_attempts += 1
          delay = [2 ** @reconnect_attempts, 30].min
          puts "Attempting reconnect in #{delay} second..."
          sleep(delay)
        end
      end
    rescue => e
      puts "Connection failed: #{e.message}"
    ensure
      # Cleanup resources if needed
      @ws = nil
    end
  end

  private

  def parse_cli_options(argv)
    options = {}

    OptionParser.new do |opts|
      opts.banner = 'Usage: multi-qemu-docker [options]'

      opts.on('-n HOSTNAME', '--name HOSTNAME', 'Hostname for reporting') { |v| options[:hostname] = v }
      opts.on('-t TYPE', '--tbox-type TYPE', 'Testbox type (vm/dc, or both by default)') { |v| options[:tbox_type] = v }
      opts.on('--max-qemu COUNT', 'Number of QEMU instances (0 for auto-computing)') { |v| options[:max_qemu] = v }
      opts.on('--max-container COUNT', 'Number of container instances (0 for auto-computing)') { |v| options[:max_container] = v }
      opts.on('-t tags', '--tags TAGS', 'separated by ","') { |v| options[:tags] = v.split(',') }
      opts.on('-r', '--remote', 'Testbox is remote') { options[:is_remote] = true }
      opts.on('-h', '--help', 'Show this message') { puts opts; exit }
    end.parse!(argv)

    options
  end

  def compute_max_vm
    host_cpu = Etc.nprocessors
    host_mem = total_memory_mb >> 10

    nr_vm_on_cpu = host_cpu / MIN_QEMU_CPU
    nr_vm_on_mem = host_mem / MIN_QEMU_MEMORY

    puts "nr_vm_on_cpu: #{nr_vm_on_cpu}"
    puts "nr_vm_on_mem: #{nr_vm_on_mem}"

    max_vm = [nr_vm_on_cpu, nr_vm_on_mem].min
  end

  def compute_max_dc
    Etc.nprocessors / MIN_CONTAINER_CPU
  end

  def build_scheduler_ws_url
    if @options[:is_remote]
      "wss://#{DOMAIN_NAME}/scheduler/v1/vm-container-provider/#{@options[:hostname]}"
    else
      "ws://#{SCHED_HOST}:#{SCHED_PORT}/scheduler/v1/vm-container-provider/#{@options[:hostname]}"
    end
  end

  def send_status
    now = Time.now
    return if @prev_io_timestamp && now - @prev_io_timestamp < 1

    # Pre-calculate values
    cpu_stats = cpu_metrics(now)
    disk_usage = disk_max_used_percent
    network_stats = network_metrics(now)

    status = {
      type: 'host-job-request',
      hostname: @options[:hostname],
      arch: ARCH,
      nr_cpu: NR_CPU,
      nr_node: NR_NODE,
      nr_disks: NR_DISKS,
      nr_vm: JobTracker.nr_vm,
      nr_container: JobTracker.nr_container,
      tbox_type: @options[:tbox_type],
      is_remote: @options[:is_remote],
      tags: @options[:tags],
      cache_dirs: @cache_dirs,

      services: { # must be string values
        sched_host: SCHED_HOST,
        sched_port: SCHED_PORT.to_s,
        result_host: SCHED_HOST,
        result_port: SCHED_PORT.to_s,
      },

      freemem: free_memory_mb, # duplicate for fast access in HostRequest
      disk_max_used_string: disk_usage[:string], # "92% /srv/os"
      metrics: { # must be number values
        freemem: free_memory_mb,
        freemem_percent: (100 * free_memory_mb) / total_memory_mb, # Integer division
        disk_max_used_percent: disk_usage[:value], # 92
        cpu_idle_percent: cpu_stats[:idle],
        cpu_iowait_percent: cpu_stats[:iowait],
        cpu_system_percent: cpu_stats[:system],
        disk_io_util_percent: disk_io_utilization(now),
        network_util_percent: network_stats[:utilization],
        network_errors_per_sec: network_stats[:errors],
        uptime_minutes: (File.read('/proc/uptime').split[0].to_i / 60), # Integer division
      }
    }
    # pp status
    @ws.send(status.to_json)
  end

  # report status every 10 seconds
  def start_heartbeat
    EM.add_periodic_timer(HEARTBEAT_INTERVAL) { send_status }
  end

  def start_cache_reclaim
    EM.add_periodic_timer(24*3600) { reclaim_cache_dirs(ENV["CACHE_DIR"], ENV["PACKAGE_CACHE_DIR"]) }
    EM.add_periodic_timer(3600) {
      reclaim_stale_locks(CACHE_LOCK_ROOT)

      base_cache_dir = ENV["CACHE_DIR"]
      full_cache_dirs = collect_cache_dirs(base_cache_dir, false)
      @cache_dirs = full_cache_dirs.map { |dir| dir.remove_prefix("#{base_cache_dir}/") }
    }
  end

  def start_mem_monitoring
    EM.add_periodic_timer(MEM_CHECK_INTERVAL) do
      @last_meminfo = parse_meminfo
      current_mem = free_memory_mb
      if (current_mem - @last_free_mem).abs > MEM_DELTA_THRESHOLD || Time.now - @last_status_time > HEARTBEAT_INTERVAL
        @last_free_mem = current_mem
        send_status
        @last_status_time = Time.now
      end
    end
  end

  def handle_websocket_message(data)
    message = JSON.parse(data)
    puts "Received: #{data}" if message['type'] != 'console-input'
    case message['type']
    when 'boot-job'
      boot_job(message)
    when 'terminate-job'
      terminate_job(message)
    when 'watch-job-log'
      watch_log(message)
    when 'unwatch-job-log'
      unwatch_log(message)
    when 'request-console'
      start_console_session(message)
    when 'console-input'
      write_to_console(message)
    when 'close-console'
      close_console_session(message)
    when 'resize-console'
      handle_console_resize(message)
    else
      puts "Unknown message type: #{message['type']}"
    end
  end

  def handle_console_resize(message)
    job_id = message['job_id']
    unless job_id
      puts "Error: Missing job_id in resize console message"
      return
    end

    job = @jobs[job_id]
    unless job
      puts "Error: Job #{job_id} not found"
      return
    end

    unless job[:console_io]
      puts "Error: No console IO available for job #{job_id}"
      return
    end

    rows = message['rows']
    cols = message['cols']

    unless rows && cols
      puts "Error: Missing rows or cols in resize message for job #{job_id}"
      return
    end

    # Update stored window size
    job[:winsize] = { rows: rows, cols: cols }
    # puts "Updated window size for job #{job_id}: #{rows}x#{cols}"

    # Set new window size if we have a PTY
    if job[:docker_pid]
      begin
        set_winsize(job[:console_io], rows, cols)
        puts "Successfully resized PTY for job #{job_id} to #{rows}x#{cols}"
      rescue => e
        puts "Error resizing PTY for job #{job_id}: #{e.message}"
        puts e.backtrace.join("\n")
      end
    else
      puts "Warning: No Container PID found for job #{job_id}. Cannot resize PTY."
    end
  end

  def set_winsize(fd, rows, cols)
    return unless fd
    buf = [rows, cols, 0, 0].pack('S!*')
    fd.ioctl(0x5414, buf)  # TIOCSWINSZ
  rescue => e
    puts "Error setting window size: #{e}"
  end

  def boot_job(message)
    job_id = message['job_id']
    puts "Dispatching job: #{message}"

    hostname = @jobs.find_hostname(message['tbox_group'])
    message['hostname'] = hostname

    host_dir = "#{ENV["HOSTS_DIR"]}/#{hostname}"
    log_file = "#{ENV["LOGS_DIR"]}/#{hostname}"
    FileUtils.rm_rf(host_dir) if Dir.exist?(host_dir)
    FileUtils.mkdir_p(host_dir + "/result_root")
    File.open(log_file, "w") {} # create before possible tail

    fork_pid = Process.fork do
      ENV["hostname"] = hostname
      ENV["host_dir"] = host_dir
      ENV["log_file"] = log_file
      ENV["is_remote"] = @options[:is_remote].to_s

      begin
        case message['tbox_type']
        when 'vm', 'qemu'
          QemuManager.new(message).start_qemu_instance
        when 'dc', 'container'
          DockerManager.new(message).start_container_instance
        end
      rescue => e
        puts "Error starting instance: #{e.message}"
        puts e.backtrace.join("\n")
      end
    end

    if fork_pid
      Process.detach(fork_pid)
      job = {
        id: job_id,
        job_token: message['job_token'],
        result_root: message['result_root'],
        tbox_type: message['tbox_type'],
        hostname: hostname,
        fork_pid: fork_pid,
        console_io: nil,
        console_thread: nil,
        docker_pid: nil,
        winsize: { rows: 24, cols: 80 } # Add initial window size
      }
      @jobs.add_job(job)
    end
  end

  def terminate_job(message)
    job_id = message['job_id']
    puts "Terminating job: #{job_id}"
    job = @jobs.remove_job(job_id)
    unless job
      puts "terminate_job: no job for job_id #{job_id}"
      return
    end

    case job[:tbox_type]
    when "qemu"
      terminate_qemu(job)
    when "container"
      terminate_container(job)
    end
    Process.kill(0, job[:fork_pid])
  end

  def terminate_qemu(job)
    qemu_pidfile = "#{PIDS_DIR}/qemu-${job[:hostname].pid}"
    qemu_pid = File.read(qemu_pidfile)
    Process.kill("INT", qemu_pid)
  end

  def terminate_container(job)
    system("#{@container_runtime} kill -s KILL #{job[:hostname]}")
  end

  def watch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    log_file = "#{ENV["LOGS_DIR"]}/#{job[:hostname]}"

    # Create a pipe to capture the output of the tail process
    reader, writer = IO.pipe

    # Start the tail process using spawn
    tail_pid = spawn("tail --pid #{job[:fork_pid]} -F #{log_file}", out: writer)
    # Process.detach(tail_pid) # avoid zombie processes

    # Store the tail process PID and pipe reader
    @jobs[job_id][:tail_pid] = tail_pid
    @jobs[job_id][:tail_reader] = reader

    # Read from the pipe in a separate thread
    Thread.new do
      begin
        while line = reader.gets
          # Send log output to WebSocket
          break unless @ws
          @ws.send({ type: 'job-log', job_id: job_id, data: line }.to_json)
        end
      rescue IOError => e
        puts "Error reading from tail process for job #{job_id}: #{e.message}"
      ensure
        reader.close
        writer.close
        Process.wait(tail_pid) # avoid zombie processes
      end
    end
  end

  def unwatch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]

    if job && job[:tail_pid]
      # Terminate the tail process
      Process.kill("TERM", job[:tail_pid])

      # Close the pipe
      if job[:tail_reader]
        job[:tail_reader].close
        job.delete(:tail_reader)
      end

      # Clean up the stored PID
      job.delete(:tail_pid)
      puts "Stopped log watching for job #{job_id}"
    else
      puts "No log watching process found for job #{job_id}"
    end
  end

  def start_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    unless job
      puts "start_console_session: no job for job_id #{job_id}"
      return
    end
    tbox_type = job[:tbox_type]

    if job[:console_io]
      puts "job already has console_io, closing old one"
    end

    rows = message['rows']
    cols = message['cols']
    job[:winsize] = { rows: rows, cols: cols }

    case tbox_type
    when 'vm', 'qemu'
      start_qemu_console(job_id, job)
    when 'dc', 'docker'
      start_docker_console(job_id, job)
    else
      puts "Error: unknown tbox_type #{tbox_type}"
    end
  end

  def wait_for_qemu_to_start(socket_path)
    # Maximum wait time in seconds (e.g., 60 seconds)
    max_wait_time = 60
    # Interval between checks in seconds (e.g., 1 second)
    check_interval = 1

    start_time = Time.now

    # Loop until the socket file is created or the max wait time is reached
    while !File.exist?(socket_path)
      if Time.now - start_time > max_wait_time
        puts "Timeout waiting for socket file to be created: #{socket_path}"
        return false
      end

      # Log the waiting status (optional)
      # @logger.info("Waiting for socket file to be created: #{socket_path}")

      # Sleep for the specified interval before checking again
      sleep(check_interval)
    end

    return true
  end

  def start_qemu_console(job_id, job)
    socket_path = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/qemu-console.sock"
    return unless wait_for_qemu_to_start(socket_path)
    begin
      socket = UNIXSocket.new(socket_path)
      job[:console_io] = socket

      # Thread to read from QEMU socket and send output
      job[:console_thread] = Thread.new do
        begin
          loop do
            data = socket.readpartial(1024)
            data = data.force_encoding("BINARY")
            base64_data = Base64.strict_encode64(data)
            @ws.send({ type: 'console-output', job_id: job_id, data: base64_data }.to_json)
          end
        rescue EOFError, Errno::EIO
          # Process exited normally
          puts "QEMU console closed"
          @ws.send({ type: 'console-exit', job_id: job_id }.to_json)
        rescue => e
          puts "QEMU console read error: #{e}"
          @ws.send({ type: 'console-error', job_id: job_id, message: e.message }.to_json)
        ensure
          job.delete(:console_io)
        end
      end
    rescue => e
      puts "Failed to attach to QEMU console: #{e.message}"
    end
  end

  # Wait until the container is running
  def wait_for_container_to_start(container_id)
    loop do
      cmd = "#{@container_runtime} inspect -f '{{.State.Status}}"
      puts cmd
      status = `#{cmd}' #{container_id}`.chomp
      break if status == "running"
      sleep 1 # Wait for 1 second before checking again
    end
  end

  def start_docker_console(job_id, job)
    container_id = job[:hostname]
    job[:console_io] = nil

    begin
      job[:console_thread] = Thread.new do
        # Wait for the container to start
        wait_for_container_to_start(container_id)

        PTY.spawn(@container_runtime, "exec", "-it", container_id, "/bin/bash", "-c", "stty raw echo isig 2>/dev/null; bash") do |output, input, pid|
          job[:console_io] = input
          job[:docker_pid] = pid
          puts "#{@container_runtime} exec -it #{container_id} /bin/bash"

          # Set initial window size
          set_winsize(output, job[:winsize][:rows], job[:winsize][:cols])

          # Disable buffering in the PTY
          output.sync = true
          input.sync = true
          output.binmode
          input.binmode
          puts "console startup"

          @ws.send({ type: 'console-startup', job_id: job_id }.to_json)

          begin
            loop do
              data = output.readpartial(1024)
              data = data.force_encoding("BINARY")
              base64_data = Base64.strict_encode64(data)
              # puts "#{Time.now.to_f} console-output: #{data.inspect}"
              @ws.send({ type: 'console-output', job_id: job_id, data: base64_data }.to_json)
            end
          rescue EOFError, Errno::EIO
            # Process exited normally
            puts "Container console closed"
            @ws.send({ type: 'console-exit', job_id: job_id }.to_json)
          rescue => e
            puts "Container console read error: #{e}"
            @ws.send({ type: 'console-error', job_id: job_id, message: e.message }.to_json)
          ensure
            input.close rescue nil
            output.close rescue nil
            job.delete(:console_io)
            job.delete(:docker_pid)
          end
        end
      end
    rescue => e
      puts "Failed to start Container console: #{e}"
      @ws.send({ type: 'console-error', job_id: job_id, message: e.message }.to_json)
    end
  end

  def write_to_console(message)
    job_id = message['job_id']
    data = message['data']
    binary_data = Base64.strict_decode64(data)
    # puts "console-input: #{binary_data}"
    job = @jobs[job_id]
    unless job
      puts "write_to_console: no job for job_id #{job_id}"
      @ws.send({ type: 'console-exit', job_id: job_id }.to_json)
      return
    end
    unless job[:console_io]
      puts "write_to_console: no console_io for job_id #{job_id}"
      @ws.send({ type: 'console-exit', job_id: job_id }.to_json)
      return
    end

    begin
      job[:console_io].write(binary_data)
      job[:console_io].flush
    rescue => e
      puts "Error writing to console: #{e}"
    end
  end

  def close_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    begin
      job[:console_io].close
    rescue => e
      puts "Error closing console IO: #{e}"
    end

    if job[:docker_pid]
      begin
        Process.kill('TERM', job[:docker_pid])
      rescue => e
        puts "Error killing Container process: #{e}"
      end
    end

    job[:console_thread].kill if job[:console_thread]
    job.delete(:console_io)
    job.delete(:console_thread)
    job.delete(:docker_pid)
  end

  def update_code(commit_id)
    # if there is no commit_id
    # the code is not updated
    return unless commit_id

    dir = "#{ENV['CCI_SRC']}/in-pull"
    return unless File.exist? dir
    FileUtils.mkdir(dir) rescue return

    cmd = "cd #{ENV['CCI_SRC']};git pull;git reset --hard #{commit_id}"
    puts cmd
    system(cmd)
  ensure
    FileUtils.rmdir(dir)
  end

  def safe_stop
    # Graceful shutdown logic
    EventMachine.stop
  end

  public

  def run
    loop do
      start_event_machine
      sleep 1
    end
  end

end

if __FILE__ == $PROGRAM_NAME
  begin
    MultiQemuDocker.new(ARGV).run
  rescue Interrupt
    exit(0)
  end
end
