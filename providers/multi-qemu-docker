#!/usr/bin/env ruby

require 'optparse'
require 'etc'
require 'json'
require 'yaml'
require 'fcntl'
require 'fileutils'
require 'find'
require 'open3'
require 'pty'
require 'shellwords'
require 'websocket/driver'

# Load common libraries
require_relative 'lib/jwt'
require_relative 'lib/cache'
require_relative 'lib/download'
require_relative 'lib/metrics'
require_relative 'lib/job_tracker'
require_relative 'docker/docker'
require_relative 'qemu/qemu'
require_relative '../container/defconfig'

ARCH = `arch`.chomp
NR_NODE = Dir.glob('/sys/devices/system/node/node*').count
NR_CPU = Etc.nprocessors
NR_DISKS = count_disks_sys_block

IS_ROOT_USER = Process.uid.zero?
BASE_DIR = IS_ROOT_USER ? "/var/lib/docker/compass-ci" : "#{ENV['HOME']}/.cache/compass-ci"
CACHE_LOCK_ROOT = "#{BASE_DIR}/.locks"

# Configuration options, the first found file takes effect
PROVIDER_CONFIG_FILES = [
  "#{ENV["HOME"]}/.config/compass-ci/provider/config.yaml",
  "/etc/compass-ci/provider/config.yaml",
]

PROVIDER_DEFAULT_OPTIONS = {
  hostname: ENV['HOST'] || ENV['HOSTNAME'] || `hostname`.chomp,
  tbox_type: 'vm,dc', # must be one of 'vm', 'dc', 'vm,dc'
  oci_runtime: "podman docker", # find use the first available one
  package_cache: true, # can speedup local develop and small deployment; disable if you have dedicated apt/dnf proxy cache services
  max_qemu: 0,
  max_container: 0,
  is_remote: false,
  tags: [],
  services: { # must be string values
    "sched_host" => '172.17.0.1',
    "sched_port" => '3000',
    "result_host" => '172.17.0.1',
    "result_port" => '3000',
  },
}.freeze

class MultiQemuDocker

  def sched_host;  @options[:services]['sched_host'] end
  def sched_port;  @options[:services]['sched_port'] end
  def result_host; @options[:services]['result_host'] end
  def result_port; @options[:services]['result_port'] end

  def load_options(argv)
    # Parse command-line options
    cli_options = parse_cli_options(argv)

    # Load options from config file
    config_options = load_config_file(cli_options)

    # Merge options in order of precedence:
    # 1. Command-line options (highest priority)
    # 2. Environment variables
    # 3. Configuration file options
    # 4. Default options (lowest priority)
    @options = PROVIDER_DEFAULT_OPTIONS
      .merge(config_options)
      .merge(cli_options)

    @options = add_env_options(@options)
    errors = validate_options(@options)
    unless errors.empty?
      puts errors.join "\n"
      exit(1)
    end

    @options
  end

  def add_env_options(config)
    config[:services] ||= {}
    config[:services]['sched_host'] = ENV['SCHED_HOST'] if ENV['SCHED_HOST']
    config[:services]['sched_port'] = ENV['SCHED_PORT'] if ENV['SCHED_PORT']
    config[:services]['result_host'] = ENV['RESULT_HOST'] if ENV['RESULT_HOST']
    config[:services]['result_port'] = ENV['RESULT_PORT'] if ENV['RESULT_PORT']
    config
  end

  # Load configuration from the first available file in PROVIDER_CONFIG_FILES
  def load_config_file(cli_options)
    if cli_options[:config_file]
      config_file = cli_options[:config_file]
    else
      config_file = PROVIDER_CONFIG_FILES.find { |file| File.exist?(file) }
    end
    return {} unless config_file

    options = YAML.load_file(config_file) || {}
    options.transform_keys(&:to_sym)
  rescue => e
    warn "Failed to load config file: #{e.message}"
    exit(1)
  end

  def parse_cli_options(argv)
    options = {}

    OptionParser.new do |opts|
      opts.banner = 'Usage: multi-qemu-docker [options]'

      opts.on('-c CONFIGFILE', '--config-file CONFIGFILE', 'config file path') { |v| options[:config_file] = v }
      opts.on('-n HOSTNAME', '--name HOSTNAME', 'Hostname for reporting') { |v| options[:hostname] = v }
      opts.on('-t TYPE', '--tbox-type TYPE', 'Testbox Type ("vm" or "dc" or "vm,dc" by default)') { |v| options[:tbox_type] = v }
      opts.on('-T TAGS', '--tags TAGS', 'tags separated by ","') { |v| options[:tags] = v.split(',') }
      opts.on('--max-qemu COUNT', 'Number of QEMU instances (0 for auto-computing)') { |v| options[:max_qemu] = v }
      opts.on('--max-container COUNT', 'Number of container instances (0 for auto-computing)') { |v| options[:max_container] = v }
      opts.on('-r', '--remote', 'Testbox is remote') { options[:is_remote] = true }
      opts.on('-h', '--help', 'Show this message') { puts opts; exit }
    end.parse!(argv)

    options
  end

  def validate_options(options)
    errors = []

    # Validate hostname
    unless options[:hostname].is_a?(String)
      errors << "hostname must be a String, but got #{options[:hostname].inspect} (#{options[:hostname].class})"
    end

    # Validate tbox_type
    valid_tbox_types = ['vm', 'dc', 'vm,dc']
    unless valid_tbox_types.include?(options[:tbox_type])
      errors << "tbox_type must be one of #{valid_tbox_types.join(', ')}, but got #{options[:tbox_type].inspect} (#{options[:tbox_type].class})"
    end

    # Validate max_qemu
    unless options[:max_qemu].is_a?(Integer)
      errors << "max_qemu must be an Integer, but got #{options[:max_qemu].inspect} (#{options[:max_qemu].class})"
    end

    # Validate max_container
    unless options[:max_container].is_a?(Integer)
      errors << "max_container must be an Integer, but got #{options[:max_container].inspect} (#{options[:max_container].class})"
    end

    # Validate is_remote
    unless options[:is_remote].is_a?(TrueClass) || options[:is_remote].is_a?(FalseClass)
      errors << "is_remote must be a Boolean, but got #{options[:is_remote].inspect} (#{options[:is_remote].class})"
    end

    # Validate tags
    unless options[:tags].is_a?(Array)
      errors << "tags must be an Array, but got #{options[:tags].inspect} (#{options[:tags].class})"
    end

    # Validate services
    unless options[:services].is_a?(Hash)
      errors << "services must be a Hash, but got #{options[:services].inspect} (#{options[:services].class})"
    else
      options[:services].each do |key, value|
        unless value.is_a?(String)
          errors << "services[#{key}] must be a String, but got #{value.inspect} (#{value.class})"
        end
      end
    end

    errors
  end

  def compute_max_vm
    host_cpu = Etc.nprocessors
    host_mem = total_memory_mb >> 10

    nr_vm_on_cpu = host_cpu / MIN_QEMU_CPU
    nr_vm_on_mem = host_mem / MIN_QEMU_MEMORY

    # puts "nr_vm_on_cpu: #{nr_vm_on_cpu}"
    # puts "nr_vm_on_mem: #{nr_vm_on_mem}"

    max_vm = [nr_vm_on_cpu, nr_vm_on_mem].min
  end

  def compute_max_dc
    Etc.nprocessors / MIN_CONTAINER_CPU
  end

end

class MultiQemuDocker
  attr_accessor :url, :thread

  def start_io_loop
    @dead = false
    @url  = build_scheduler_ws_url
    uri = URI.parse(@url)

    begin
    @tcp  = TCPSocket.new(uri.host, uri.port)
    @tcp.setsockopt(Socket::IPPROTO_TCP, Socket::TCP_NODELAY, 1)

    @driver = WebSocket::Driver.client(self)

    @driver.on(:open)    { |event| handle_websocket_open }
    @driver.on(:message) { |event| handle_websocket_message(event) }
    @driver.on(:close)   { |event| handle_websocket_close(event) }
    @driver.on(:error)   { |event| handle_websocket_error(event) }

    if @options[:is_remote]
      jwt = load_jwt?
      @driver.set_header('Authorization', jwt)
    end

    start_tcp_reader
    @driver.start
    @tcp_read_thread.join
    rescue => e
      puts "Connection error: #{e.message}"
      @tcp.close if @tcp && !@tcp.closed?
    end
  end

  def build_scheduler_ws_url
    if @options[:is_remote]
      "wss://#{sched_host}:#{sched_port}/scheduler/v1/vm-container-provider/#{@options[:hostname]}"
    else
      "ws://#{sched_host}:#{sched_port}/scheduler/v1/vm-container-provider/#{@options[:hostname]}"
    end
  end

  def send_json(hash)
    @driver.text(JSON.pretty_generate(hash))
  end

  def write(data)
    begin
      @tcp.write(data)
    rescue Errno::EPIPE => e
      puts "TCP write: #{e.message}"
      close_all
    end
  end

  def start_tcp_reader
    @threads << @tcp_read_thread = Thread.new do
      loop do
        break if @dead
        data = @tcp.read_nonblock(1 << 16)
        # data = @tcp.readpartial(102400)
        @driver.parse(data)
      rescue IO::WaitReadable
        IO.select([@tcp])
        retry
      rescue => e
        puts "Socket read error: #{e.message}"
        break
      end
    end
  end

end

class MultiQemuDocker

  MIN_QEMU_CPU = 2
  MIN_CONTAINER_CPU = 2
  MIN_QEMU_MEMORY = 4       # GB
  MIN_CONTAINER_MEMORY = 8  # GB

  HEARTBEAT_INTERVAL = 10   # seconds
  MEM_CHECK_INTERVAL = 1    # second
  MEM_DELTA_THRESHOLD = 100 # 100MB

  JOB_DONE_FIFO_PATH = "/tmp/job_completion_fifo"

  attr_reader :options
  attr_accessor :last_free_mem, :last_status_time

  def initialize(argv)
    load_options(argv)
    @last_meminfo = parse_meminfo
    @last_free_mem = free_memory_mb
    @last_status_time = Time.now
    setup_directories
    update_options
    prepare_busybox
    @jobs = JobTracker.new
    @container_runtime = @options[:oci_runtime].split.find { |runtime| system("command -v #{runtime} > /dev/null 2>&1") }
    @cache_dirs = []
    @threads = []
  end

  def setup_directories
    ENV["BASE_DIR"] = BASE_DIR
    FileUtils.mkdir_p(ENV["HOSTS_DIR"] = "#{BASE_DIR}/provider/hosts")
    FileUtils.mkdir_p(ENV["JOBS_DIR"] = "#{BASE_DIR}/provider/jobs")
    FileUtils.mkdir_p(ENV["PIDS_DIR"] = "#{BASE_DIR}/provider/pids")
    FileUtils.mkdir_p(ENV["LOGS_DIR"] = "#{BASE_DIR}/provider/logs")
    FileUtils.mkdir_p(ENV["DOWNLOAD_DIR"] = "#{BASE_DIR}/provider/download")

    # for unpacked cpio packages
    FileUtils.mkdir_p(ENV["PKG_STORE_DIR"] = "#{BASE_DIR}/provider/pkgstore")

    # File.umask(0000) # Ensure directories are created with proper permissions

    FileUtils.mkdir_p(CACHE_LOCK_ROOT)
    FileUtils.mkdir_p(ENV["CACHE_DIR"] = "#{BASE_DIR}/provider/cache")

    # for rpm/deb packages
    if @options[:package_cache]
      FileUtils.mkdir_p(ENV["PACKAGE_CACHE_DIR"] = "#{BASE_DIR}/provider/pkgcache")
    end
  end

  def prepare_busybox
    return unless @options[:tbox_type].include?('dc')

    # Define possible busybox paths
    busybox_paths = [
      "/srv/file-store/busybox/#{ARCH}",
      "#{BASE_DIR}/file-store/busybox/#{ARCH}"
    ]

    # Check if busybox exists in any location
    return if busybox_paths.find { |path| File.exist?(path + "/busybox") }

    busybox_path = busybox_paths.last
    puts "Preparing #{busybox_path}"
    FileUtils.mkdir_p(busybox_path)

    # Download and install busybox
    busybox_url = "http://#{sched_host}:#{sched_port}#{busybox_paths.first}/busybox"
    busybox_file = "#{busybox_path}/busybox"

    unless system("wget --timeout=30 --tries=3 -nv #{busybox_url} -O #{busybox_file}")
      raise "Failed to download busybox from #{busybox_url}"
    end

    FileUtils.chmod('+x', busybox_file)

    Dir.chdir(busybox_path) do
      unless system("./busybox --install .")
        raise "Failed to install busybox in #{busybox_path}"
      end
    end
  end

  def handle_websocket_open
    puts "WebSocket connection opened"
    send_status
    start_heartbeat
    start_cache_reclaim
    start_fifo_listener
    start_mem_monitoring
  end

  def handle_websocket_close(event)
    puts "Connection closed with code #{event.code}"
    close_all
  rescue => e
    puts "Error during connection close: #{e.message}"
  end

  def handle_websocket_error(event)
    puts "WebSocket error: #{event.message}"
    puts "Please check if server is running on #{@url}" if event.message == Errno::ECONNREFUSED
    close_all
  end

  def close_all
    @dead = true

    # Clean up threads
    @threads.each do |t|
      t.kill if t.alive?
      t.join rescue nil
    end
    @threads.clear

    # Close network resources
    @driver.close rescue nil
    if @tcp && !@tcp.closed?
      @tcp.close rescue nil
    end
    @driver.close
  end

  def handle_websocket_message(event)
    begin
      message = JSON.parse(event.data)
      puts "Received: #{event.data}" if message['type'] != 'console-input'
      case message['type']
      when 'boot-job'
        boot_job(message)
      when 'terminate-job'
        terminate_job(message)
      when 'watch-job-log'
        watch_log(message)
      when 'unwatch-job-log'
        unwatch_log(message)
      when 'request-console'
        start_console_session(message)
      when 'console-input'
        write_to_console(message)
      when 'close-console'
        close_console_session(message)
      when 'resize-console'
        handle_console_resize(message)
      else
        puts "Unknown message type: #{message['type']}"
      end
    rescue => e
      puts "Message handling error: #{e.message}"
      puts e.backtrace.join("\n")
    end
  end

  def update_options
    if @options[:max_qemu] == 0
      @options[:max_qemu] = compute_max_vm
    end
    if @options[:max_container] == 0
      @options[:max_container] = compute_max_dc
    end
  end

  def create_fifo
    unless File.exist?(JOB_DONE_FIFO_PATH)
      system('mkfifo', JOB_DONE_FIFO_PATH)
    end
  end

  def start_fifo_listener
    create_fifo

    @threads << Thread.new do
      loop do
        break if @dead
        # Need re-open FIFO for block reading, since the other side always
        # close FIFO after writing one line of job_id.
        File.open(JOB_DONE_FIFO_PATH, 'r') do |fifo|
          job_id = fifo.gets
          unless job_id
            puts "Unexpected end of input from the FIFO."
            next
          end
          case job_id.chomp
            when /^boot: (\d+)/
              job_id = $1
              status = {
                type: 'job-update',
                job_id: job_id,
                job_stage: "boot",
              }
              send_json(status)
            when /^done: (\d+)/
              job_id = $1
              if job = @jobs.remove_job(job_id)
                upload_log(job)
                upload_results(job)
              else
                puts "Cannot find job #{job_id}"
              end
              status = {
                type: 'job-update',
                job_id: job_id,
                job_data_readiness: "uploaded",  # trigger extract-stats
              }
              send_json(status)

              puts "Job #{job_id} completed and removed from the job list."
          end
        end
      end
    end

  end

  # ruby version of $LKP_SRC/lib/upload.sh upload_one()
  def upload_one(job, local_file, remote_path)
      # URL encode the remote path (important for spaces, etc.)
      encoded_remote_path = remote_path.gsub(/([^a-zA-Z0-9_.\/-])/) { |match| "%#{match.ord.to_s(16).upcase}" }

      # Construct the target path
      target_path = File.join(job[:result_root], encoded_remote_path)

      # Construct the HTTP URL
      http_url = "http://#{result_host}:#{result_port}#{target_path}?job_id=#{job[:id]}&job_token=#{job[:job_token]}"

      # Maximum retries and retry delay
      max_retries = 50
      retry_delay = 5
      retry_count = 0

      # Read the file content in binary mode
      file_content = File.binread(local_file)

      # Retry loop
      until @dead
        # Use Open3 to execute curl with stdin_data
        stdout, stderr, status = Open3.capture3(
          'curl', '-s', '--fail-with-body', '-X', 'POST', '-w', '\ncode=%{http_code}', '--data-binary', '@-', http_url,
          stdin_data: file_content
        )

        # Extract the HTTP status code from the output
        response_code = stdout.match(/code=(\d+)/)[1].to_i

        case response_code
        when 200, 201
          break
        when 400..600
          puts "Upload failed: #{local_file} to #{target_path}"
          puts stdout
          break
        else
          if retry_count < max_retries
            puts "Upload failed (#{stdout}). Retrying in #{retry_delay} seconds..."
            sleep retry_delay
            retry_count += 1
            retry_delay *= 2
          else
            puts "Upload failed after #{max_retries} attempts"
            break
          end
        end
      end
  end

  def upload_log(job)
    upload_one(job, "#{ENV["LOGS_DIR"]}/#{job[:hostname]}", "console.log")
  end

  def upload_results(job)
    # Construct the base directory path
    base_dir = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/result_root/"

    # Recursively find all files under the base directory
    Find.find(base_dir) do |local_file|
      next unless File.file?(local_file) # Skip directories, only process files

      # Construct the remote path by preserving the local pathname/filename under result_root
      remote_path = local_file.sub(base_dir, '')

      upload_one(job, local_file, remote_path)
    end
  end


  private

  def send_status
    now = Time.now
    return if @prev_io_timestamp && now - @prev_io_timestamp < 1

    # Pre-calculate values
    cpu_stats = cpu_metrics(now)
    disk_usage = disk_max_used_percent
    network_stats = network_metrics(now)

    status = {
      type: 'host-job-request',
      hostname: @options[:hostname],
      arch: ARCH,
      nr_cpu: NR_CPU,
      nr_node: NR_NODE,
      nr_disks: NR_DISKS,
      nr_vm: JobTracker.nr_vm,
      nr_container: JobTracker.nr_container,
      tbox_type: @options[:tbox_type],
      is_remote: @options[:is_remote],
      tags: @options[:tags],
      cache_dirs: @cache_dirs,

      services: @options[:services],

      freemem: free_memory_mb, # duplicate for fast access in HostRequest
      disk_max_used_string: disk_usage[:string], # "92% /srv/os"
      metrics: { # must be number values
        freemem: free_memory_mb,
        freemem_percent: (100 * free_memory_mb) / total_memory_mb, # Integer division
        disk_max_used_percent: disk_usage[:value], # 92
        cpu_idle_percent: cpu_stats[:idle],
        cpu_iowait_percent: cpu_stats[:iowait],
        cpu_system_percent: cpu_stats[:system],
        disk_io_util_percent: disk_io_utilization(now),
        network_util_percent: network_stats[:utilization],
        network_errors_per_sec: network_stats[:errors],
        uptime_minutes: (File.read('/proc/uptime').split[0].to_i / 60), # Integer division
      }
    }
    # pp status
    send_json(status)
  end

  # report status every 10 seconds
  def start_heartbeat
    @threads << Thread.new do
      until @dead
        sleep(HEARTBEAT_INTERVAL)
        send_status
      end
    end
  end

  def start_cache_reclaim
    @threads << Thread.new do
      hours = 0
      until @dead
        sleep(3600)

        hours += 1
        if hours >= 24
          reclaim_cache_dirs
          hours = 0
        end

        reclaim_stale_locks(CACHE_LOCK_ROOT)

        # @cache_dirs updated here will be send_status() to scheduler
        base_cache_dir = ENV["CACHE_DIR"]
        full_cache_dirs = collect_cache_dirs(base_cache_dir, false)
        @cache_dirs = full_cache_dirs.map { |dir| dir.delete_prefix("#{base_cache_dir}/") }

      end
    rescue => e
      puts "Cache reclaim error: #{e.message}"
    end
  end

  def start_mem_monitoring
    @threads << Thread.new do
      until @dead
        sleep(MEM_CHECK_INTERVAL)
        @last_meminfo = parse_meminfo
        current_mem = free_memory_mb
        if (current_mem - @last_free_mem).abs > MEM_DELTA_THRESHOLD || Time.now - @last_status_time > HEARTBEAT_INTERVAL
          @last_free_mem = current_mem
          send_status
          @last_status_time = Time.now
        end
      end
    end
  end

  def handle_console_resize(message)
    job_id = message['job_id']
    unless job_id
      puts "Error: Missing job_id in resize console message"
      return
    end

    job = @jobs[job_id]
    unless job
      puts "Error: Job #{job_id} not found"
      return
    end

    unless job[:console_io]
      puts "Error: No console IO available for job #{job_id}"
      return
    end

    rows = message['rows']
    cols = message['cols']

    unless rows && cols
      puts "Error: Missing rows or cols in resize message for job #{job_id}"
      return
    end

    # Update stored window size
    job[:console_rows] = rows
    job[:console_cols] = cols
    # puts "Updated window size for job #{job_id}: #{rows}x#{cols}"

    # Set new window size if we have a PTY
    if job[:docker_pid]
      begin
        if set_winsize(job[:console_io], rows, cols)
          puts "Successfully resized PTY for job #{job_id} to #{rows}x#{cols}"
        end
      rescue => e
        puts "Error resizing PTY for job #{job_id}: #{e.message}"
        puts e.backtrace.join("\n")
      end
    else
      # no ioctl for QEMU UNIX socket, we only send stty command on initial connect
      # puts "Warning: No Container PID found for job #{job_id}. Cannot resize PTY."
    end
  end

  def set_winsize(fd, rows, cols)
    return unless fd
    buf = [rows, cols, 0, 0].pack('S!*')
    fd.ioctl(0x5414, buf)  # TIOCSWINSZ
  rescue => e
    puts "Error setting window size: #{e}"
  end

  def boot_job(message)
    job_id = message['job_id']
    puts "Dispatching job: #{message}"

    hostname = @jobs.find_hostname(message['tbox_group'])
    message['hostname'] = hostname

    host_dir = "#{ENV["HOSTS_DIR"]}/#{hostname}"
    log_file = "#{ENV["LOGS_DIR"]}/#{hostname}"
    FileUtils.rm_rf(host_dir) if Dir.exist?(host_dir)
    FileUtils.mkdir_p(host_dir + "/result_root")
    File.open(log_file, "w") {} # create before possible tail

    fork_pid = Process.fork do
      ENV["hostname"] = hostname
      ENV["host_dir"] = host_dir
      ENV["log_file"] = log_file
      ENV["is_remote"] = @options[:is_remote].to_s

      begin
        case message['tbox_type']
        when 'vm', 'qemu'
          QemuManager.new(message).start_qemu_instance
        when 'dc', 'container'
          ENV["OCI_RUNTIME"] = @container_runtime
          DockerManager.new(message).start_container_instance
        end
      rescue => e
        puts "Error starting instance: #{e.message}"
        puts e.backtrace.join("\n")
        @jobs.remove_job(job_id)
        status = {
          type: 'job-update',
          job_id: job_id,
          job_stage: "abort_provider",
          job_health: "abort_provider",
          job_data_readiness: "norun",
        }
        send_json(status)
      end
    end

    if fork_pid
      Process.detach(fork_pid)
      job = {
        id: job_id,
        job_token: message['job_token'],
        result_root: message['result_root'],
        tbox_type: message['tbox_type'],
        hostname: hostname,
        fork_pid: fork_pid,
        console_io: nil,
        console_thread: nil,
        docker_pid: nil,
        console_rows: 24,
        console_cols: 80,
      }
      @jobs.add_job(job)
    end
  end

  def terminate_job(message)
    job_id = message['job_id']
    puts "Terminating job: #{job_id}"
    job = @jobs.remove_job(job_id)
    unless job
      puts "terminate_job: no job for job_id #{job_id}"
      return
    end

    case job[:tbox_type]
    when "qemu"
      terminate_qemu(job)
    when "container"
      terminate_container(job)
    end
    Process.kill(0, job[:fork_pid])
  rescue => e
    puts "Error terminating job #{job[:id]}: #{e.message}"
  end

  def terminate_qemu(job)
    qemu_pidfile = "#{ENV['PIDS_DIR']}/qemu-#{job[:hostname]}.pid"
    return unless File.exist?(qemu_pidfile)

    qemu_pid = File.read(qemu_pidfile).strip.to_i
    Process.kill("INT", qemu_pid)
  rescue => e
    puts "Error terminating QEMU: #{e.message}"
  end

  def terminate_container(job)
    system("#{@container_runtime} kill -s KILL #{job[:hostname]}")
  end

  def watch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    unless job
      puts "watch_log: no job for job_id #{job_id}"
      return
    end
    log_file = "#{ENV["LOGS_DIR"]}/#{job[:hostname]}"

    # Create a pipe to capture the output of the tail process
    reader, writer = IO.pipe

    # Start the tail process using spawn
    tail_pid = spawn("tail --pid #{job[:fork_pid]} -F #{log_file}", out: writer)
    # Process.detach(tail_pid) # avoid zombie processes

    # Store the tail process PID and pipe reader
    job[:tail_pid] = tail_pid
    job[:tail_reader] = reader

    # Read from the pipe in a separate thread
    Thread.new do
      begin
        while line = reader.gets
          # Send log output to WebSocket
          send_json({ type: 'job-log', job_id: job_id, data: line })
          # When console started, auto stop log to avoid mess up the screen
          # especially necessary to avoid double output lines when there is 1
          # single QEMU -serial.
          break if job[:console_io]
          break if @dead
        end
      rescue Errno::ESRCH => e
        puts "Tail process already exited: #{e.message}"
      rescue IOError => e
        puts "tail process exit for job #{job_id}: #{e.message}"
      ensure
        job.delete :tail_pid
        job.delete :tail_reader
        reader.close
        writer.close
        Process.wait(tail_pid) # avoid zombie processes
        send_json({ type: 'log-exit', job_id: job_id })
      end
    end
  end

  def unwatch_log(message)
    job_id = message['job_id']
    job = @jobs[job_id]

    if job && job[:tail_pid]
      # Terminate the tail process
      Process.kill("TERM", job[:tail_pid]) rescue nil

      # Close the pipe
      if job[:tail_reader]
        job[:tail_reader].close
        job.delete(:tail_reader)
      end

      # Clean up the stored PID
      job.delete(:tail_pid)
      puts "Stopped log watching for job #{job_id}"
    else
      puts "No log watching process found for job #{job_id}"
    end
  end

  def start_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    unless job
      puts "start_console_session: no job for job_id #{job_id}"
      send_json({ type: 'console-exit', job_id: job_id })
      return
    end
    tbox_type = job[:tbox_type]

    if job[:console_io]
      puts "job already has console_io, closing old one"
    end

    job[:console_rows] = message['rows']
    job[:console_cols] = message['cols']

    case tbox_type
    when 'vm', 'qemu'
      start_qemu_console(job_id, job)
    when 'dc', 'docker'
      start_docker_console(job_id, job)
    else
      puts "Error: unknown tbox_type #{tbox_type}"
    end
  end

  def wait_for_qemu_to_start(socket_path)
    # Maximum wait time in seconds (e.g., 60 seconds)
    max_wait_time = 60
    # Interval between checks in seconds (e.g., 1 second)
    check_interval = 1

    start_time = Time.now

    # Loop until the socket file is created or the max wait time is reached
    while !File.exist?(socket_path)
      if Time.now - start_time > max_wait_time
        puts "Timeout waiting for socket file to be created: #{socket_path}"
        return false
      end

      # Log the waiting status (optional)
      # @logger.info("Waiting for socket file to be created: #{socket_path}")

      # Sleep for the specified interval before checking again
      sleep(check_interval)
    end

    return true
  end

  def start_qemu_console(job_id, job)
    # Thread to read from QEMU socket and send output
    job[:console_thread] = Thread.new do
      socket_path = "#{ENV["HOSTS_DIR"]}/#{job[:hostname]}/qemu-console.sock"
      return unless wait_for_qemu_to_start(socket_path)

      socket = UNIXSocket.new(socket_path)
      socket.write("\r\nstty rows #{job[:console_rows]} cols #{job[:console_cols]}\r\n")
      job[:console_io] = socket
      send_json({ type: 'console-startup', job_id: job_id })

      until @dead
        data = socket.readpartial(1024)
        data = data.force_encoding("BINARY")
        base64_data = Base64.strict_encode64(data)
        send_json({ type: 'console-output', job_id: job_id, data: base64_data })
      end
    rescue EOFError, Errno::EIO
      # Process exited normally
      puts "QEMU console closed"
      send_json({ type: 'console-exit', job_id: job_id })
    rescue => e
      puts "QEMU console read error: #{e}"
      send_json({ type: 'console-error', job_id: job_id, message: e.message })
    ensure
      job.delete(:console_io)
    end
  end

  # Wait until the container is running
  def wait_for_container_to_start(job_id, container_id)
    secs = 0
    until @dead
      cmd = "#{@container_runtime} inspect -f '{{.State.Status}}"
      # puts cmd
      status = `#{cmd}' #{container_id} 2>/dev/null`.chomp
      break if status == "running"
      break unless @jobs[job_id]
      secs += 1
      sleep 1 # Wait for 1 second before checking again
      break if secs > 3600
    end
  end

  def start_docker_console(job_id, job)
    container_id = job[:hostname]
    job[:console_io] = nil

    begin
      job[:console_thread] = Thread.new do
        # Wait for the container to start
        wait_for_container_to_start(job_id, container_id)

        PTY.spawn(@container_runtime, "exec", "-it", container_id, "/bin/bash", "-c", "stty raw echo opost icrnl isig 2>/dev/null; bash") do |output, input, pid|
          job[:console_io] = input
          job[:docker_pid] = pid
          puts "#{@container_runtime} exec -it #{container_id} /bin/bash"

          # Set initial window size
          set_winsize(output, job[:console_rows], job[:console_cols])

          # Disable buffering in the PTY
          output.sync = true
          input.sync = true
          output.binmode
          input.binmode
          puts "console startup"

          send_json({ type: 'console-startup', job_id: job_id })

          begin
            until @dead
              data = output.readpartial(1024)
              data = data.force_encoding("BINARY")
              base64_data = Base64.strict_encode64(data)
              # puts "#{Time.now.to_f} console-output: #{data.inspect}"
              send_json({ type: 'console-output', job_id: job_id, data: base64_data })
            end
          rescue EOFError, Errno::EIO
            # Process exited normally
            puts "Container console closed"
            send_json({ type: 'console-exit', job_id: job_id })
          rescue => e
            puts "Container console read error: #{e}"
            send_json({ type: 'console-error', job_id: job_id, message: e.message })
          ensure
            input.close rescue nil
            output.close rescue nil
            job.delete(:console_io)
            job.delete(:docker_pid)
          end
        end
      end
    rescue => e
      puts "Failed to start Container console: #{e}"
      send_json({ type: 'console-error', job_id: job_id, message: e.message })
    end
  end

  def write_to_console(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    unless job
      puts "write_to_console: no job for job_id #{job_id}"
      send_json({ type: 'console-exit', job_id: job_id })
      return
    end
    unless job[:console_io]
      puts "write_to_console: no console_io for job_id #{job_id}"
      send_json({ type: 'console-exit', job_id: job_id })
      return
    end

    data = message['data']
    binary_data = Base64.strict_decode64(data)
    # puts "console-input: #{binary_data.inspect}"
    begin
      job[:console_io].write(binary_data)
      job[:console_io].flush
    rescue => e
      puts "Error writing to console: #{e}"
    end
  end

  def close_console_session(message)
    job_id = message['job_id']
    job = @jobs[job_id]
    return unless job
    return unless job[:console_io]

    begin
      job[:console_io].close
    rescue => e
      puts "Error closing console IO: #{e}"
    end

    if job[:docker_pid]
      begin
        Process.kill('TERM', job[:docker_pid])
      rescue => e
        puts "Error killing Container process: #{e}"
      end
    end

    if job[:console_thread]
      job[:console_thread].kill rescue nil
      job[:console_thread].join
    end
    job.delete(:console_thread)
    job.delete(:console_io)
    job.delete(:docker_pid)
  end

  def update_code(commit_id)
    # if there is no commit_id
    # the code is not updated
    return unless commit_id

    dir = "#{ENV['CCI_SRC']}/in-pull"
    return unless File.exist? dir
    FileUtils.mkdir(dir) rescue return

    cmd = "cd #{ENV['CCI_SRC']};git pull;git reset --hard #{commit_id}"
    puts cmd
    system(cmd)
  ensure
    FileUtils.rmdir(dir)
  end

end

def run
  tries = 0
  loop do
    provider = MultiQemuDocker.new(ARGV)
    provider.start_io_loop

    tries += 1
    delay = [2 ** tries, 30].min
    puts "Attempting reconnect in #{delay} second..."
    sleep(delay)
  end
end

if __FILE__ == $PROGRAM_NAME
  begin
    run
  rescue Interrupt
    exit(0)
  end
end
